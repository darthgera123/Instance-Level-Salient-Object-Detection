# -*- coding: utf-8 -*-
"""cvproj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1di-AolJNQG4IY9xdsZWeQJqT7nWZLY
"""

# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import os
import cv2

import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torchvision
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torchvision.models as models
from PIL import Image
import random
from tensorboardX import SummaryWriter
import time

## configs
SCALE_1 = (160, 160)
SCALE_2 = (120, 120)
SCALE_3 = (80, 80)

LEARNING_RATE = 1e-4

BATCH_SIZE = 8

NUM_EPOCHS = 10

TRAIN_IMAGES = 'drive/My Drive/cv_dataset/train/images/'
TRAIN_MASKS = 'drive/My Drive/cv_dataset/train/masks/'

VAL_IMAGES = 'drive/My Drive/cv_dataset/val/images/'
VAL_MASKS = 'drive/My Drive/cv_dataset/val/masks/'

VALIDATION_FREQ = 300

VALID_LOSS_MIN = np.Inf
TRAIN_LOSSES =[]
VALID_LOSSES =[]

EXP_NAME = "full_dataset_5000"
SAVE_DIR = "full_dataset"

tb = SummaryWriter()

def flip_dataset(dir_path):
  imgs = sorted([item for item in os.listdir(dir_path)])
  i = 0
  for img in imgs:
    i += 1
    img_path = dir_path+img
    img_name, extension = img[:-4], img[-4:]
    print(i, img_name, extension)
    image_flip = cv2.flip(cv2.imread(img_path), 1)
    flip_name = img_name + '_flip'
    name = dir_path + flip_name + extension
    # print(name)
    cv2.imwrite(name, image_flip)
    # cv2.imwrite(dir_path + flip_name + '.' + extension, image_flip)

def change_extension(dir_path, new_extension):
  imgs = sorted([item for item in os.listdir(dir_path)])
  print(len(imgs))
  i = 0
  for img in imgs:
    print(i)
    i += 1
    img_path = dir_path+img
    img_name, extension = img[:-4], img[-3:]
    if extension != new_extension:
      print(img_path, img_name, extension , 'to', new_extension)
      image = cv2.imread(img_path)
      cv2.imwrite(dir_path + img_name + '.' + new_extension, image)
      os.remove(img_path)

# change_extension(VAL_IMAGES, 'png')

def plot_images(images, rows, columns, fig_size):
      
    i = 0
    num = len(images)
    
    if num == 1:
        plt.figure(figsize=fig_size)
        plt.imshow(images[0])
        plt.show()
        return
    
    _, plts = plt.subplots(rows, columns, figsize = fig_size)

    for r in range(rows):
        for c in range(columns):
            if i < num:
                if rows == 1 or columns == 1:
                    plts[r+c].imshow(images[i], cmap='gray')
                else:
                    plts[r][c].imshow(images[i], cmap='gray')
                i = i+1
    plt.show()
    return

def save_checkpoint(state,filename):
  torch.save(state,filename)

def visualize(gt,mask,output,epoch, exp_name, tb, tag):
  final = np.zeros((3,160,160*3))
  gt1 = torch.squeeze(gt.detach(), dim=0)
  gt1 = gt1[1].numpy()
  m1 = torch.squeeze(mask.detach(), dim=0)
  m1 = m1[1].cpu().numpy()
  output1 = torch.squeeze(output.detach(), dim=0)
  output1 = output1[1].cpu().numpy()
  final[:,:,320:480] = (gt1*255).astype('uint8')
  final[:,:,:160] = m1.astype('uint8')
  final[:,:,160:320] = output1.astype('uint8')

  # tb.add_image('%s/%s/ground_truth' % (exp_name, tag),(gt1*255).astype('uint8') , epoch)
  tb.add_image('%s/%s/mask,output,image' % (exp_name, tag),final , epoch)

class Salient(Dataset):

  def __init__(self,samples,img_dir,mask_dir,img_transforms=None,mask_transforms=None,):
    super(Dataset,self).__init__()
    self.img_transforms = img_transforms
    self.mask_transforms = mask_transforms

    self.img_path = img_dir
    self.mask_path = mask_dir

    if samples == -1:
      self.img_dir = sorted([item for item in os.listdir(img_dir)])
      self.mask_dir = sorted([item for item in os.listdir(mask_dir)])
    else:
      self.img_dir = sorted([item for item in os.listdir(img_dir)])[:samples]
      self.mask_dir = sorted([item for item in os.listdir(mask_dir)])[:samples]

    self.check_dataset()
    
  def check_dataset(self):
    # print(len(self.img_dir), len(self.mask_dir))
    if self.img_dir == self.mask_dir:
      print('consistent')
      return True  
    else:
      print('in-consistent')
      return False

  def __len__(self):
	  return len(self.img_dir)

  def __getitem__(self,index):
	
    img =  cv2.imread(self.img_path+self.img_dir[index])
    # print(self.img_dir[index])
    img1 = cv2.resize(img, SCALE_1)
    img2 = cv2.resize(img, SCALE_2)
    img3 = cv2.resize(img, SCALE_3)
  
    mask = cv2.imread(self.mask_path+self.mask_dir[index])
    mask = cv2.resize(mask, SCALE_1)
  
    gt=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)

    if self.img_transforms != None:
      img1 = self.img_transforms(img1) 
      img2 = self.img_transforms(img2) 
      img3 = self.img_transforms(img3) 
	 
    if self.mask_transforms != None:
      mask = self.mask_transforms(mask)
      gt = self.mask_transforms(gt)

    return img1,img2,img3,mask,gt

img_transforms = transforms.Compose([
                                      transforms.ToPILImage(),
                                      transforms.ToTensor(),
                                      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))
                                      ])
mask_transforms = transforms.Compose([
                                      transforms.ToPILImage(),
                                      transforms.ToTensor(),
                                      ])

train_data = Salient(-1,TRAIN_IMAGES,TRAIN_MASKS,img_transforms,mask_transforms)
val_data = Salient(-1,VAL_IMAGES,VAL_MASKS,img_transforms,mask_transforms)

# train_data = Salient("small/",50,train_transforms,True)
print(train_data.__len__(),val_data.__len__())

class RefinedVGG16(nn.Module):
	def __init__(self):
		super(RefinedVGG16,self).__init__()
		self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.MaxPool2d(kernel_size=3,stride=2))
		self.conv2 = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.MaxPool2d(kernel_size=3,stride=2))
		self.conv3 = nn.Sequential(nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.MaxPool2d(kernel_size=3,stride=2))
		self.conv4 = nn.Sequential(nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1),
									nn.ReLU(),
									nn.MaxPool2d(kernel_size=3,stride=1,padding=1))
		self.conv5 = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=2,dilation=2),
									nn.ReLU(),
									nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=2,dilation=2),
									nn.ReLU(),
									nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=2,dilation=2),
									nn.ReLU(),
									nn.MaxPool2d(3,stride=1,padding=1))
		self.fc6 = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3,padding=12,dilation=12),
								nn.ReLU(),
								nn.Dropout(p=0.5))
		self.fc7 = nn.Sequential(nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=1),
								nn.ReLU(),
								nn.Dropout(p=0.5))
		self.mask1 = nn.Sequential(nn.Conv2d(in_channels=1024,out_channels=64,kernel_size=1,stride=1),
		                            nn.ReLU())
		self.skip1 = nn.Sequential(nn.Conv2d(in_channels=1024,out_channels=64,kernel_size=3,padding=1),
		                            nn.ReLU())
		self.mask2 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.skip2 = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask3 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.skip3 = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask4 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.skip4 = nn.Sequential(nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask5 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask5_up = nn.Sequential(nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=4,stride=2,padding=1,output_padding=1),
		                              nn.ReLU())
		self.skip5 = nn.Sequential(nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask6 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask6_up = nn.Sequential(nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=4,stride=2,padding=1,output_padding=1),
		                              nn.ReLU())
		self.skip6 = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),
		                           nn.ReLU())
		self.mask7 = nn.Sequential(nn.Conv2d(in_channels=64*2,out_channels=64,kernel_size=3,padding=1),
                           		nn.ReLU())
		self.mask7_up = nn.Sequential(nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=4,stride=2),
		                              nn.ReLU())
		

	def forward(self,x):
		self.size = x.shape[-1]
		# print('self.size:', x.shape)
		self.fb1 = self.conv1(x)
		# print('self.fb1:', self.fb1.shape)
		self.fb2 = self.conv2(self.fb1)
		# print('self.fb2:', self.fb2.shape)
		self.fb3 = self.conv3(self.fb2)
		# print('self.fb3:', self.fb3.shape)
		self.fb4 = self.conv4(self.fb3)
		# print('self.fb4:', self.fb4.shape)
		self.fb5 = self.conv5(self.fb4)
		# print('self.fb5:', self.fb5.shape)

		self.fb6 = self.fc7(self.fc6(self.fb5))
		# print('self.fb6:', self.fb6.shape)
		self.td1 = self.skip1(self.fb6)
		# print('self.td1:', self.td1.shape)
		
		self.fbu1 = self.mask1(self.fb6)
		# print('self.fbu1:', self.fbu1.shape)
		self.r1 = torch.cat((self.td1, self.fbu1), 1)
		# print('self.r1:', self.r1.shape)
		
		self.td2 = self.mask2(self.r1)
		# print('self.td2:', self.td2.shape)
		self.fbu2 = self.skip2(self.fb5)
		# print('self.fbu2:', self.fbu2.shape)
		self.r2 = torch.cat((self.td2,self.fbu2),1)
		# print('self.r2:', self.r2.shape)

		self.td3 = self.mask3(self.r2)
		# print('self.td3:', self.td3.shape)
		self.fbu3 = self.skip3(self.fb4)
		# print('self.fbu3:', self.fbu3.shape)
		self.r3 = torch.cat((self.td3,self.fbu3),1)
		# print('self.r3:', self.r3.shape)
		
		self.td4 = self.mask4(self.r3)
		# print('self.td4:', self.td4.shape)
		self.fbu4 = self.skip4(self.fb3)
		# print('self.fbu4:', self.fbu4.shape)
		self.r4 = torch.cat((self.td4,self.fbu4),1)
		# print('self.r4:', self.r4.shape)
		
		self.td5 = self.mask5(self.r4)
		self.td5 = self.mask5_up(self.td5)
		# print('self.td5:', self.td5.shape)
		self.fbu5 = self.skip5(self.fb2)
		# print('self.fbu5:', self.fbu5.shape)
		self.r5 = torch.cat((self.td5,self.fbu5),1)
		
		self.td6 = self.mask6(self.r5)
		self.td6 = self.mask6_up(self.td6)
		self.fbu6 = self.skip6(self.fb1)
		self.r6 = torch.cat((self.td6,self.fbu6),1)
	
		self.td7 = self.mask7(self.r6)
		self.td7 = self.mask7_up(self.td7)

		# print('self.td7:', self.td7.shape)	
		return self.td7

class Attention(nn.Module):
  def __init__(self, in_channels):
    super(Attention,self).__init__()
    self.conv1 = nn.Sequential(nn.Conv2d(in_channels=in_channels,out_channels=512,kernel_size=3,padding=1),
                               nn.ReLU(),
                               nn.Dropout(p=0.5))
                              
    self.conv2 = nn.Sequential(nn.Conv2d(in_channels=512,out_channels=3,kernel_size=3,padding=1),
                               nn.Softmax2d())
    
  def forward(self,x):
    x = self.conv1(x)
    x = self.conv2(x)
    attention1 = torch.unsqueeze(x[:,0,:,:], 1)
    attention2 = torch.unsqueeze(x[:,1,:,:], 1)
    attention3 = torch.unsqueeze(x[:,2,:,:], 1)

    # print(x.shape)
    return attention1, attention2, attention3

class Classifier(nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    self.fc8 = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=1,kernel_size=1))

  def forward(self, x):
    return self.fc8(x)

class Region(nn.Module):
  def __init__(self):
    super(Region,self).__init__()
    self.l1 = RefinedVGG16()
    self.l2 = RefinedVGG16()
    self.l3 = RefinedVGG16()
    self.output_mask = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=1,kernel_size=3,stride=1,padding=1))
    self.attention = Attention(3)
    self.attention_fc7 = Attention(64*3)
    self.clf1 = Classifier()
    self.clf2 = Classifier()
    self.clf3 = Classifier()
  

  def forward(self,x1,x2,x3):
    output1 = self.l1(x1)
    output2 = self.l2(x2)
    output2_interp = F.interpolate(output2, SCALE_1,mode='bilinear',align_corners=True)
    output3 = self.l3(x3)
    output3_interp = F.interpolate(output3, SCALE_1,mode='bilinear',align_corners=True)

    # print('ckpt 1')
    output1_fc8 = self.clf1(output1)
    output2_fc8 = self.clf2(output2)
    output2_fc8 = F.interpolate(output2_fc8, SCALE_1,mode='bilinear',align_corners=True)
    output3_fc8 = self.clf3(output3)
    output3_fc8 = F.interpolate(output3_fc8, SCALE_1,mode='bilinear',align_corners=True)
    # print('ckpt 2')
    attention_fc7_inp = torch.cat((output1, output2_interp, output3_interp),1)
    # print(output1.shape, attention_fc7_inp.shape)
    attention1_fc7, attention2_fc7, attention3_fc7 = self.attention_fc7(attention_fc7_inp)
    # print('ckpt 3')

    output_mask1 = self.output_mask(self.l1(x1)) 
    output_mask2 = self.output_mask(self.l2(x2))
    output_mask3 = self.output_mask(self.l3(x3))
    output_mask2_interp = F.interpolate(output_mask2, SCALE_1,mode='bilinear',align_corners=True)
    output_mask3_interp = F.interpolate(output_mask3, SCALE_1,mode='bilinear',align_corners=True)
   
    # sending them through attention lanext(iter(data_loader))yer
    attention1, attention2, attention3 = self.attention(torch.cat((output_mask1,output_mask2_interp,output_mask3_interp),1))
    # print(attention1.shape, attention2.shape, attention3.shape)
    # print(attention1.shape, output_mask1.shape)
    output_fusion = attention1*output_mask1 + attention2*output_mask2_interp + attention3*output_mask3_interp
    fc8_fusion = attention1_fc7*output1_fc8 + attention2_fc7*output2_fc8 + attention3_fc7*output3_fc8
    fc8_fusion_interp = F.interpolate(fc8_fusion, SCALE_1,mode='bilinear',align_corners=True)
    
    # output = nn.Sigmoid()(output_fusion + fc8_fusion_interp)
    output = output_fusion + fc8_fusion_interp
    
    return torch.cat((output,output,output),1)

train_on_gpu = torch.cuda.is_available()
if train_on_gpu:
    print("Yes Gpu is on")
else:
    print("Time to sleep")

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
valid_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)

def custom_init(m):
  for k, v in m.named_parameters():
    # print(k)
    if 'mask' in k or 'skip' in k:
      if 'weight' in k:
        print(k, "initialised as normal with mean=0.0 and std=0.01")
        nn.init.normal_(v, mean=0.0, std=0.01)
      else:
        print(k, "initialised with zeros")
        nn.init.zeros_(v)

model = Region()
if train_on_gpu:
    model.cuda()
custom_init(model)

pos_weight = torch.ones([160])*2
# no of classes
error = nn.BCEWithLogitsLoss(pos_weight=pos_weight.cuda())

layers_lr_1 = []
layers_lr_2 = []
rem_layers = []
for k, v in model.named_parameters():
  if 'mask7' not in k:
    if 'l1.conv' in k or 'l2.conv' in k or 'l3.conv' in k:
      # print(k)
      if 'weight' in k:
        layers_lr_1.append(v)
      else:
        layers_lr_2.append(v)
    else:
      rem_layers.append(v)

# opt = optim.Adam([{'params': rem_layers}, 
#                   {'params': layers_lr_1, 'lr': learning_rate, 'weight_decay': 0.0005},
#                   {'params': layers_lr_2, 'lr': 2*learning_rate, 'weight_decay': 0},
#                   {'params': list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in ['l1.mask7.0.weight','l2.mask7.0.weight','l3.mask7.0.weight'], model.named_parameters())))), 'lr': 10*learning_rate, 'weight_decay': 0.0005},
#                   {'params': list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in ['l1.mask7.0.bias','l2.mask7.0.bias','l3.mask7.0.bias'], model.named_parameters())))), 'lr': 20*learning_rate, 'weight_decay': 0},], 
#                  lr=learning_rate, weight_decay=0.0005)
# opt = optim.SGD(model.parameters(),lr=1e-3,weight_decay=0.0005,momentum=0.9)
opt = optim.SGD([{'params': rem_layers}, 
                  {'params': layers_lr_1, 'lr': LEARNING_RATE, 'weight_decay': 0.0005},
                  {'params': layers_lr_2, 'lr': 2*LEARNING_RATE, 'weight_decay': 0},
                  {'params': list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in ['l1.mask7.0.weight','l2.mask7.0.weight','l3.mask7.0.weight'], model.named_parameters())))), 'lr': 10*LEARNING_RATE, 'weight_decay': 0.0005},
                  {'params': list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in ['l1.mask7.0.bias','l2.mask7.0.bias','l3.mask7.0.bias'], model.named_parameters())))), 'lr': 20*LEARNING_RATE, 'weight_decay': 0},], 
                 lr=LEARNING_RATE, weight_decay=0.0005, momentum=0.9)
scheduler = lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=20, min_lr=1e-6)

vgg16 = models.vgg16(pretrained=True)

vgg_dict = vgg16.state_dict()
model_dict = model.l1.state_dict()
pretrained_dict = {}

pretrained_req = []
for k, v in vgg_dict.items():
  if k == 'classifier.0.weight':
    break
  pretrained_req.append((k, v))

cnt = 0
for k, v in model_dict.items():
  if cnt == len(pretrained_req):
    break
  print(k, 'initiaised with', pretrained_req[cnt][0])
  pretrained_dict[k] = pretrained_req[cnt][1]
  cnt += 1

model_dict.update(pretrained_dict)
model.l1.load_state_dict(model_dict)
model.l2.load_state_dict(model_dict)
model.l3.load_state_dict(model_dict)

print("Start the training")

def validate(epoch):
  valid_loss = 0.0
  with torch.no_grad():
      model.eval()
      opt.zero_grad()
      for img1,img2,img3,mask,gt in valid_loader:
        if train_on_gpu:
          img1,img2,img3,mask = img1.cuda(),img2.cuda(),img3.cuda(),mask.cuda()

        
        output = model(img1.type(torch.cuda.FloatTensor),img2.type(torch.cuda.FloatTensor),(img3.type(torch.cuda.FloatTensor)))
        loss = error(output,mask)
        valid_loss += loss.item()*img1.size(0)
        
      if valid_loss <= VALID_LOSS_MIN:
        print("Validation Loss decreased {:0.6f} -> {:0.6f}".format(VALID_LOSS_MIN, valid_loss))
        VALID_LOSS_MIN = valid_loss
        save_checkpoint({
				'epoch': epoch,
				'state_dict': model.state_dict(),
				'optimizer': opt.state_dict(),
			}, '%s/%s.pt' % (SAVE_DIR, EXP_NAME))
    
  print('Epoch: {} \tValidation Loss: {:.6f}'.format(
          epoch, valid_loss))


def train(num_epochs):
  for epoch in range(1, num_epochs):
    start = time.time()
    print("Epoch starting",epoch)
    train_loss = 0.0
    i = 0
    for timg1,timg2,timg3,tmask,tgt in train_loader:
      i += 1
      if i%10 ==0:
        print('step:', i)
      model.train()
      opt.zero_grad()
      if train_on_gpu:
        timg1,timg2,timg3,tmask = timg1.cuda(),timg2.cuda(),timg3.cuda(),tmask.cuda()
      
      # getting all 3 probability maps
      toutput = model(timg1.type(torch.cuda.FloatTensor),timg2.type(torch.cuda.FloatTensor),(timg3.type(torch.cuda.FloatTensor)))

      loss = error(toutput,tmask)
      loss.backward()
      if i%2 ==0:
        opt.step()
      train_loss += loss.item()*timg1.size(0)

      if epoch % VALIDATION_FREQ == 0:
        validate(epoch)
      print('train loss:', train_loss)
      TRAIN_LOSSES.append(train_loss)
      tb.add_scalar('%s/train_loss' % EXP_NAME, train_loss, epoch)
      tb.add_scalar('%s/validation_loss' % EXP_NAME, valid_loss, epoch)

      visualize(tgt,tmask,toutput, epoch, EXP_NAME, tb, 'train_data')
      visualize(gt,mask,output, epoch, EXP_NAME, tb, 'validation_data')

    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
          epoch, train_loss))
    end = time.time()
    print("time taken ",end-start)


train(NUM_EPOCHS)


# tb.close()

def visualize_model_results(path_to_model, samples, split, dataset_folder):
  test_model = Region()
  test_model = test_model.cuda()
  test_model.load_state_dict(torch.load(path_to_model))
  

  img_transforms = transforms.Compose([
                                      transforms.ToPILImage(),
                                      transforms.ToTensor(),
                                      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))
                                      ])
  mask_transforms = transforms.Compose([
                                      transforms.ToPILImage(),
                                      transforms.ToTensor(),
                                      ])

  test_data = Salient(16000,TRAIN_IMAGES,TRAIN_MASKS,img_transforms,mask_transforms)

  test_data.__len__()
  batch = 1
  test_loader = DataLoader(test_data, batch_size = batch)
  i = 0
  with torch.no_grad():
    model.eval()
    for img1, img2, img3, mask, gt in test_loader:
      if i == samples:
        break
      i += 1
      
      img1,img2,img3,mask = img1.cuda(),img2.cuda(),img3.cuda(),mask.cuda()

      output = test_model(img1.type(torch.cuda.FloatTensor),img2.type(torch.cuda.FloatTensor),(img3.type(torch.cuda.FloatTensor)))
      
      output = output.squeeze()
      mask = mask.squeeze()
      
      output = output.detach().cpu().numpy()
      mask = mask.cpu().numpy()

      output = np.transpose(output, (1,2,0))
      mask = np.transpose(mask, (1,2,0))

      print(mask.shape, output.shape, mask.dtype, output.dtype)


      plot_images([mask, output], 1, 2, (20,20))
    

# torch.save(model.state_dict(), 'drive/My Drive/check.pt')
# visualize_model_results('drive/My Drive/check.pt', 10, 10,'drive/My Drive/small/')

